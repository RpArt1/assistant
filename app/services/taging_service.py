from typing import List
from pydantic import BaseModel, Field
import json
from openai import OpenAI
from os import environ
import logging
import traceback
from ..utils import file_processor
from ..utils.enums import  TagEnum
from .open_ai_service import call_ai


class TagModel(BaseModel):
    pre_defined_tags: List[TagEnum] = Field(..., description="Tags from pre-defined list matching document")
    generated_tags: List[str] = Field(..., description="Additional tags generated by open ai")


# Create two possible functions to choose from: 
function_list = [
    {
        "name" : "tag-document",
        "description" : "Function responsible for returning list of tags matching document content",
        "parameters" : TagModel.model_json_schema(),
        "required": ["pre_defined_tags", "generated_tags"]
    },
]


def tag_document(content: str, mock: bool = False) -> List:
    """ Tags docuemnt

    Args:
        message (str): document content
        mock (bool) : flag used for development purposes if call to external system is not required
    """
    if mock: 
        return mock_response()
       
    try:
        tag_prompt = file_processor.process_file("../prompts/tag_document_prompt.md")
        tags = call_ai(content, tag_prompt, function_list)
        logging.info(f"Query categorization : {tags}")
        return tags
    except Exception as e:
        raise TagingError(f"Error while taging file: {e}", 1002)





def mock_response(): 
    mock_expected_document_tags = { 'tags': ['ai', 'python']}
    mock_expected_document_tags_1 = { 'tags': ['brain', 'neurons', 'health']}

    return mock_expected_document_tags


class TagingError(Exception):
    """Exception raised for errors in the document taging process.
       error code = 1002 
    """
    def __init__(self, message, error_code):
        self.message = message
        self.error_code = error_code
        super().__init__(message)
